{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping reviews for Lenoevo Ideapad slim3...\n",
      "Login page detected. Please log in manually within 15 seconds.\n",
      "Successfully scraped page 1 for Lenoevo Ideapad slim3\n",
      "Successfully scraped page 2 for Lenoevo Ideapad slim3\n",
      "Successfully scraped page 3 for Lenoevo Ideapad slim3\n",
      "Successfully scraped page 4 for Lenoevo Ideapad slim3\n",
      "Successfully scraped page 5 for Lenoevo Ideapad slim3\n",
      "Successfully scraped page 6 for Lenoevo Ideapad slim3\n",
      "Successfully scraped page 7 for Lenoevo Ideapad slim3\n",
      "Successfully scraped page 8 for Lenoevo Ideapad slim3\n",
      "Successfully scraped page 9 for Lenoevo Ideapad slim3\n",
      "Reached last page (10) for Lenoevo Ideapad slim3. Ending scraping.\n",
      "Total reviews collected for Lenoevo Ideapad slim3: 100\n",
      "Finished scraping reviews for Lenoevo Ideapad slim3.\n",
      "\n",
      "Scraping reviews for Hp Pavilion 14-inch...\n",
      "Successfully scraped page 1 for Hp Pavilion 14-inch\n",
      "Reached last page (2) for Hp Pavilion 14-inch. Ending scraping.\n",
      "Total reviews collected for Hp Pavilion 14-inch: 17\n",
      "Finished scraping reviews for Hp Pavilion 14-inch.\n",
      "\n",
      "Scraping reviews for Macbook Air M1...\n",
      "Successfully scraped page 1 for Macbook Air M1\n",
      "Successfully scraped page 2 for Macbook Air M1\n",
      "Successfully scraped page 3 for Macbook Air M1\n",
      "Successfully scraped page 4 for Macbook Air M1\n",
      "Successfully scraped page 5 for Macbook Air M1\n",
      "Successfully scraped page 6 for Macbook Air M1\n",
      "Successfully scraped page 7 for Macbook Air M1\n",
      "Successfully scraped page 8 for Macbook Air M1\n",
      "Successfully scraped page 9 for Macbook Air M1\n",
      "Reached last page (10) for Macbook Air M1. Ending scraping.\n",
      "Total reviews collected for Macbook Air M1: 100\n",
      "Finished scraping reviews for Macbook Air M1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from urllib.parse import parse_qs, urlencode, urlparse, urlunparse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Initialize WebDriver (e.g., Chrome)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Lists of product URLs and names\n",
    "desired_prods = []\n",
    "prod_names = []\n",
    "\n",
    "# Function to update the page number in the URL\n",
    "def update_page_number(url, page_number):\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    query_params['pageNumber'] = [str(page_number)]\n",
    "    updated_query = urlencode(query_params, doseq=True)\n",
    "    updated_url = urlunparse(parsed_url._replace(query=updated_query))\n",
    "    return updated_url\n",
    "\n",
    "# Function to scrape reviews for a single product\n",
    "def scrape_reviews(url, product_name):\n",
    "    i = 0\n",
    "    names = []\n",
    "    ratings = []\n",
    "    rating_dates = []\n",
    "    titles = []\n",
    "    reviews_text = []\n",
    "\n",
    "    while url is not None:\n",
    "        try:\n",
    "            i = i + 1\n",
    "            if i > 1:\n",
    "                url = update_page_number(url, i)  # Update the URL with the correct pageNumber\n",
    "            \n",
    "            driver.get(url)\n",
    "            time.sleep(5)  # Allow the page to load\n",
    "\n",
    "            # Check if the login page is displayed\n",
    "            if \"ap/signin\" in driver.current_url:\n",
    "                print(\"Login page detected. Please log in manually within 15 seconds.\")\n",
    "                time.sleep(15)  # Wait for 15 seconds for manual login\n",
    "                driver.get(url)  # Reload the page after login\n",
    "                time.sleep(5)  # Allow the page to load\n",
    "\n",
    "            # Parse the page with BeautifulSoup\n",
    "            html_data = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            reviews = html_data.find_all('li', {'data-hook': 'review'})\n",
    "\n",
    "            # If no reviews are found, pause for 15 seconds for manual intervention\n",
    "            if not reviews:\n",
    "                print(\"No reviews found. Pausing for 15 seconds for manual intervention...\")\n",
    "                time.sleep(15)  # Wait for 15 seconds for manual intervention\n",
    "                driver.get(url)  # Reload the page\n",
    "                time.sleep(5)  # Allow the page to load\n",
    "                html_data = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                reviews = html_data.find_all('li', {'data-hook': 'review'})\n",
    "\n",
    "                # If still no reviews, skip this product\n",
    "                if not reviews:\n",
    "                    print(f\"No reviews found after retry for {product_name}. Skipping...\")\n",
    "                    break\n",
    "\n",
    "            for review in reviews:\n",
    "                name = review.find('span', {'class': 'a-profile-name'}).text\n",
    "                names.append(name.strip())\n",
    "                rating = review.find('span', {'class': 'a-icon-alt'}).text\n",
    "                ratings.append(rating)\n",
    "                rating_date = review.find('span', {'data-hook': 'review-date'}).text\n",
    "                rating_dates.append(rating_date)\n",
    "                title = review.find('a', {'data-hook': 'review-title'}).text\n",
    "                titles.append(title)\n",
    "                review_text = review.find('span', {'data-hook': 'review-body'}).text\n",
    "                reviews_text.append(review_text)\n",
    "\n",
    "            # Check for next page\n",
    "            url_check = html_data.find('li', {'class': 'a-last'})\n",
    "            if url_check is None or url_check.find('a') is None:\n",
    "                print(f\"Reached last page ({i}) for {product_name}. Ending scraping.\")\n",
    "                url = None\n",
    "            else:\n",
    "                print(f\"Successfully scraped page {i} for {product_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred on page {i} for {product_name}: {str(e)}\")\n",
    "            url = None\n",
    "\n",
    "    print(f\"Total reviews collected for {product_name}: {len(names)}\")\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    data = pd.DataFrame({\n",
    "        'profile_name': names,\n",
    "        'rating': ratings,\n",
    "        'rating_date': rating_dates,\n",
    "        'title': titles,\n",
    "        'review_text': reviews_text\n",
    "    })\n",
    "    data['product_name'] = product_name\n",
    "    data.to_csv(f'{product_name}.csv', index=False)\n",
    "\n",
    "# Iterate through each product and scrape reviews\n",
    "for url, product_name in zip(desired_prods, prod_names):\n",
    "    print(f\"Scraping reviews for {product_name}...\")\n",
    "    scrape_reviews(url, product_name)\n",
    "    print(f\"Finished scraping reviews for {product_name}.\\n\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
