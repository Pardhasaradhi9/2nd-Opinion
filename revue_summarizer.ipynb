{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neattext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chunks for CSV 1: 100%|██████████| 626/626 [00:00<00:00, 895815.18it/s]\n",
      "Joining chunks for CSV 1: 100%|██████████| 12/12 [00:00<00:00, 52211.25it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1766 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Summarizing chunks for CSV 1: 100%|██████████| 12/12 [00:36<00:00,  3.08s/it]\n",
      "Processing CSV files:  33%|███▎      | 1/3 [00:41<01:22, 41.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for /Users/pardhasaradhichukka/Desktop/Project/Reboot/pracrtise/data_sets/Hp Pavilion 14-inch.csv saved to txt1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chunks for CSV 2: 100%|██████████| 315/315 [00:00<00:00, 948460.70it/s]\n",
      "Joining chunks for CSV 2: 100%|██████████| 6/6 [00:00<00:00, 48865.68it/s]\n",
      "Summarizing chunks for CSV 2: 100%|██████████| 6/6 [00:15<00:00,  2.63s/it]\n",
      "Processing CSV files:  67%|██████▋   | 2/3 [01:01<00:28, 28.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for /Users/pardhasaradhichukka/Desktop/Project/Reboot/pracrtise/data_sets/Lenoevo Ideapad slim3.csv saved to txt2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chunks for CSV 3: 100%|██████████| 441/441 [00:00<00:00, 578569.93it/s]\n",
      "Joining chunks for CSV 3: 100%|██████████| 7/7 [00:00<00:00, 45449.11it/s]\n",
      "Summarizing chunks for CSV 3: 100%|██████████| 7/7 [00:17<00:00,  2.45s/it]\n",
      "Processing CSV files: 100%|██████████| 3/3 [01:23<00:00, 27.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for /Users/pardhasaradhichukka/Desktop/Project/Reboot/pracrtise/data_sets/Macbook Air M1.csv saved to txt3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "import neattext as nt\n",
    "\n",
    "# List of CSV file paths\n",
    "csv_paths = [\n",
    "    '/data_sets/Hp Pavilion 14-inch.csv',\n",
    "    '/data_sets/Lenoevo Ideapad slim3.csv',\n",
    "    '/data_sets/Macbook Air M1.csv'\n",
    "]\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_with_neattext(text):\n",
    "    text_frame = nt.TextFrame(text)\n",
    "    text_frame.remove_multiple_spaces()\n",
    "    text_frame.remove_html_tags()\n",
    "    text_frame.remove_stopwords()\n",
    "    text_frame.remove_non_ascii()\n",
    "    text_frame.remove_userhandles()\n",
    "    text_frame.remove_hashtags()\n",
    "    text_frame.remove_emojis()\n",
    "    return text_frame.text\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=0)\n",
    "\n",
    "# Process each CSV file\n",
    "for i, csv_path in enumerate(tqdm(csv_paths, desc=\"Processing CSV files\")):\n",
    "    # Load the CSV file\n",
    "    mbook_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Combine all rows of the 'review_text' column into a single string\n",
    "    full_desc = ' '.join(mbook_df['review_text'].dropna().astype(str))\n",
    "    \n",
    "    # Preprocess the text\n",
    "    clean_full_desc = preprocess_with_neattext(full_desc)\n",
    "    clean_full_desc = clean_full_desc.replace('.', '.<eos>')\n",
    "    clean_full_desc = clean_full_desc.replace('?', '?<eos>')\n",
    "    clean_full_desc = clean_full_desc.replace('!', '!<eos>')\n",
    "    \n",
    "    # Split text into chunks for summarization\n",
    "    max_chunk = 500\n",
    "    sentences = clean_full_desc.split('<eos>')\n",
    "    current_chunk = 0\n",
    "    chunks = []\n",
    "    \n",
    "    for sentence in tqdm(sentences, desc=f\"Creating chunks for CSV {i+1}\"):\n",
    "        if len(chunks) == current_chunk + 1:\n",
    "            if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                chunks[current_chunk].extend(sentence.split(' '))\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence.split(' '))\n",
    "        else:\n",
    "            chunks.append(sentence.split(' '))\n",
    "    \n",
    "    # Join words in each chunk to form sentences\n",
    "    for chunk_id in tqdm(range(len(chunks)), desc=f\"Joining chunks for CSV {i+1}\"):\n",
    "        chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "    \n",
    "    # Summarize each chunk\n",
    "    res = []\n",
    "    for chunk in tqdm(chunks, desc=f\"Summarizing chunks for CSV {i+1}\"):\n",
    "        summary = summarizer(chunk, max_length=140, min_length=30, do_sample=False)\n",
    "        res.extend(summary)\n",
    "    \n",
    "    # Combine all summaries into a single text\n",
    "    summary_text = ' '.join([summ['summary_text'] for summ in res])\n",
    "    \n",
    "    # Summarize the combined summary text for a final concise summary\n",
    "    final_summary = summarizer(summary_text, max_length=300, min_length=100, do_sample=False)\n",
    "    \n",
    "    # Save the final summary to a text file\n",
    "    output_file = f'txt{i+1}.txt'\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(final_summary[0]['summary_text'])\n",
    "    \n",
    "    print(f\"Summary for {csv_path} saved to {output_file}\")\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
